## Implement MMoE multi task learning model from scratch for a toy example and show that it runs as intended.
https://github.com/rameshavinash94/CMPE297_Assignments_All/blob/A3MTL%26TL/MMOE_multi_task_learning_model.ipynb 


## Head2Toe: Utilizing Intermediate Representations for Better OOD Generalization vs Traditional TL 

Head-to-Toe probing (Head2Toe), that selects features from all layers of the source model to train a classification head for the target-domain. In evaluations on the Visual Task Adaptation Benchmark (VTAB), Head2Toe matches performance obtained with fine-tuning on average while reducing training and storage cost hundred folds or more, but critically, for out-of-distribution transfer, Head2Toe outperforms fine-tuning\

Head2Toe matches performance obtained with fine-tuning on average while reducing training and storage cost a hundred fold or more, but critically, for out-of-distribution transfer, Head2Toe outperforms fine-tuning.

## DATASET: CIFAR10 : https://www.cs.toronto.edu/~kriz/cifar.html

Head2Toe:
https://github.com/rameshavinash94/CMPE297_Assignments_All/blob/A3MTL%26TL/Head2Toe.ipynb 

TL:
https://github.com/rameshavinash94/CMPE297_Assignments_All/blob/A3MTL%26TL/TransferLearningCIFAR10.ipynb

HEAD2TOE has similar/higher accuracy compared with traditional TL


TRADITIONAL TL ACCURACY:

<img width="711" alt="Screen Shot 2022-10-23 at 11 02 21 PM" src="https://user-images.githubusercontent.com/87649563/197458069-d49d2b0f-7c68-4b7d-8085-98bfaa009fab.png">
