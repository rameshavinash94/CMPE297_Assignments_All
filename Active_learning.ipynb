{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D4pRltTs7E2"
      },
      "source": [
        "!pip install lightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii1Ap9VQtsMH",
        "outputId": "35091368-7f6c-448a-d4d8-82a16a7b8d49"
      },
      "source": [
        "!git clone https://github.com/alexeygrigorev/clothing-dataset-small.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'clothing-dataset-small'...\n",
            "remote: Enumerating objects: 3839, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (400/400), done.\u001b[K\n",
            "remote: Total 3839 (delta 9), reused 385 (delta 0), pack-reused 3439\u001b[K\n",
            "Receiving objects: 100% (3839/3839), 100.58 MiB | 24.27 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYiAkU2ptzMi"
      },
      "source": [
        "## Creation dataset on the Lightly Platform with embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFEjD5sDt2-S",
        "outputId": "ba220111-ece5-40b0-feea-0b72931e0b00"
      },
      "source": [
        "!lightly-train input_dir=clothing-dataset-small/train trainer.max_epochs=1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/hydra/_internal/hydra.py:127: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  configure_logging=with_log_configuration,\n",
            "/usr/local/lib/python3.7/dist-packages/lightly/cli/train_cli.py:69: UserWarning: Training a self-supervised model with a small batch size: 16! Small batch size may harm embedding quality. You can specify the batch size via the loader key-word: loader.batch_size=BSZ\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://storage.googleapis.com/models_boris/whattolabel-resnet18-simclr-d32-w1.0-i-085d0693.pth\" to /root/.cache/torch/hub/checkpoints/whattolabel-resnet18-simclr-d32-w1.0-i-085d0693.pth\n",
            "100% 42.8M/42.8M [00:00<00:00, 194MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: /content/lightly_outputs/2022-11-16/04-46-51/lightning_logs\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:601: UserWarning: Checkpoint directory /content/lightly_outputs/2022-11-16/04-46-51 exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type       | Params\n",
            "-----------------------------------------\n",
            "0 | model     | _SimCLR    | 11.2 M\n",
            "1 | criterion | NTXentLoss | 0     \n",
            "-----------------------------------------\n",
            "11.2 M    Trainable params\n",
            "0         Non-trainable params\n",
            "11.2 M    Total params\n",
            "44.762    Total estimated model params size (MB)\n",
            "Epoch 0:  26% 50/191 [00:15<00:42,  3.31it/s, loss=3.28, v_num=0][2022-11-16 04:47:13,499][numexpr.utils][INFO] - NumExpr defaulting to 2 threads.\n",
            "Epoch 0: 100% 191/191 [00:41<00:00,  4.58it/s, loss=3.1, v_num=0]`Trainer.fit` stopped: `max_epochs=1` reached.\n",
            "Epoch 0: 100% 191/191 [00:42<00:00,  4.54it/s, loss=3.1, v_num=0]\n",
            "Best model is stored at: \u001b[94m/content/lightly_outputs/2022-11-16/04-46-51/lightly_epoch_0.ckpt\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltHXWpoquQCA",
        "outputId": "fdf30a41-5d9b-4996-a999-56d55f684c39"
      },
      "source": [
        "#Create embeddings for the dataset (replace **mycheckpoint.ckpt** with the model checkpoint from the lightly-train output), e.g. with\n",
        "\n",
        "!lightly-embed input_dir=\"clothing-dataset-small/train\" checkpoint=\"/content/lightly_outputs/2022-11-16/04-46-51/lightly_epoch_0.ckpt\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/hydra/_internal/hydra.py:127: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  configure_logging=with_log_configuration,\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "Compute efficiency: 0.15: 100% 3068/3068 [00:15<00:00, 193.20imgs/s]\n",
            "Embeddings are stored at \u001b[94m/content/lightly_outputs/2022-11-16/04-48-29/embeddings.csv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMQ34wJ8wMAG"
      },
      "source": [
        "#Save the path to the **embeddings.csv**, you will need it later for uploading the embeddings and for defining the dataset for the classifier:\n",
        "# rename the embeddings output to embeddings.csv\n",
        "!mv '/content/lightly_outputs/2022-11-16/04-48-29/embeddings.csv' 'embeddings.csv'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a new dataset on the lightly platform as described in `lightly-platform` and Save the **token** and **dataset id**, you will need them later to upload the images and embeddings and to run the active learning samplers.\n",
        "!lightly-magic token='0496e4756268346002a999825678f0be22e29c8145447c50' dataset_id='63746bfff32e72da2b44a788' input_dir='/content/clothing-dataset-small/train' trainer.max_epochs=2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj3yPYq0p-v4",
        "outputId": "23de7ea5-7913-4224-da89-bc68ff6d0fb2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/hydra/_internal/hydra.py:127: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "  configure_logging=with_log_configuration,\n",
            "########## Starting to train an embedding model.\n",
            "/usr/local/lib/python3.7/dist-packages/lightly/cli/train_cli.py:69: UserWarning: Training a self-supervised model with a small batch size: 16! Small batch size may harm embedding quality. You can specify the batch size via the loader key-word: loader.batch_size=BSZ\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
            "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: /content/lightly_outputs/2022-11-16/04-51-36/lightning_logs\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:601: UserWarning: Checkpoint directory /content/lightly_outputs/2022-11-16/04-51-36 exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type       | Params\n",
            "-----------------------------------------\n",
            "0 | model     | _SimCLR    | 11.2 M\n",
            "1 | criterion | NTXentLoss | 0     \n",
            "-----------------------------------------\n",
            "11.2 M    Trainable params\n",
            "0         Non-trainable params\n",
            "11.2 M    Total params\n",
            "44.762    Total estimated model params size (MB)\n",
            "Epoch 0:  26% 50/191 [00:10<00:28,  4.89it/s, loss=3.28, v_num=0][2022-11-16 04:51:51,560][numexpr.utils][INFO] - NumExpr defaulting to 2 threads.\n",
            "Epoch 1: 100% 191/191 [00:33<00:00,  5.69it/s, loss=2.54, v_num=0]`Trainer.fit` stopped: `max_epochs=2` reached.\n",
            "Epoch 1: 100% 191/191 [00:33<00:00,  5.62it/s, loss=2.54, v_num=0]\n",
            "Best model is stored at: \u001b[94m/content/lightly_outputs/2022-11-16/04-51-36/lightly_epoch_1.ckpt\u001b[0m\n",
            "########## Starting to embed your dataset.\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py:23: LightningDeprecationWarning: pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7 and will be removed in v1.9. Use the equivalent class from the pytorch_lightning.core.module.LightningModule class instead.\n",
            "  \"pytorch_lightning.core.lightning.LightningModule has been deprecated in v1.7\"\n",
            "Compute efficiency: 0.31: 100% 3068/3068 [00:12<00:00, 242.95imgs/s]\n",
            "Embeddings are stored at \u001b[94m/content/lightly_outputs/2022-11-16/04-51-36/embeddings.csv\u001b[0m\n",
            "########## Starting to upload your dataset to the Lightly platform.\n",
            "Uploading \u001b[92m3068\u001b[0m images (with \u001b[92m2\u001b[0m workers).\n",
            "100% 3068/3068 [10:05<00:00,  5.07imgs/s]\n",
            "Finished the upload of the dataset.\n",
            "Finished upload of embeddings.\n",
            "########## Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUYBPit0r7Iq"
      },
      "source": [
        "## Active learning\n",
        "Import the Python frameworks we need for this tutorial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y1qm4tQr7Iq"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from lightly.active_learning.agents.agent import ActiveLearningAgent\n",
        "# from lightly.active_learning.config.sampler_config import SamplerConfig\n",
        "from lightly.active_learning.scorers.classification import ScorerClassification\n",
        "from lightly.api.api_workflow_client import ApiWorkflowClient\n",
        "from lightly.openapi_generated.swagger_client import SamplingMethod"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGjxfGM8r7Iq"
      },
      "source": [
        "Define the parameters (make sure to set the path to your embeddings file).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-idlQo4r7Ir"
      },
      "source": [
        "path_to_embeddings_csv = \"/content/embeddings.csv\"\n",
        "YOUR_TOKEN = \"0496e4756268346002a999825678f0be22e29c8145447c50\"  # your token of the web platform\n",
        "YOUR_DATASET_ID = \"63746bfff32e72da2b44a788\"  # the id of your dataset on the web platform"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3OLzonXr7Ir"
      },
      "source": [
        "class CSVEmbeddingDataset:\n",
        "    def __init__(self, path_to_embeddings_csv: str):\n",
        "        with open(path_to_embeddings_csv, 'r') as f:\n",
        "            data = csv.reader(f)\n",
        "\n",
        "            rows = list(data)\n",
        "            header_row = rows[0]\n",
        "            rows_without_header = rows[1:]\n",
        "\n",
        "            index_filenames = header_row.index('filenames')\n",
        "            filenames = [row[index_filenames] for row in rows_without_header]\n",
        "\n",
        "            index_labels = header_row.index('labels')\n",
        "            labels = [row[index_labels] for row in rows_without_header]\n",
        "\n",
        "            embeddings = rows_without_header\n",
        "            indexes_to_delete = sorted([index_filenames, index_labels], reverse=True)\n",
        "            for embedding_row in embeddings:\n",
        "                for index_to_delete in indexes_to_delete:\n",
        "                    del embedding_row[index_to_delete]\n",
        "\n",
        "        # create the dataset as a dictionary mapping from the filename to a tuple of the embedding and the label\n",
        "        self.dataset: Dict[str, Tuple[np.ndarray, int]] = \\\n",
        "            dict([(filename, (np.array(embedding_row, dtype=float), int(label)))\n",
        "                  for filename, embedding_row, label in zip(filenames, embeddings, labels)])\n",
        "\n",
        "    def get_features(self, filenames: List[str]) -> np.ndarray:\n",
        "        features_array = np.array([self.dataset[filename][0] for filename in filenames])\n",
        "        return features_array\n",
        "\n",
        "    def get_labels(self, filenames: List[str]) -> np.ndarray:\n",
        "        labels = np.array([self.dataset[filename][1] for filename in filenames])\n",
        "        return labels"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JCLRhC4rA2w"
      },
      "source": [
        "!export LIGHTLY_SERVER_LOCATION=\"https://api-dev.lightly.ai\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSSA8Qcgr7Is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96585a94-8ca9-4252-cec6-dac509b5e822"
      },
      "source": [
        "os.environ['LIGHTLY_SERVER_LOCATION']=\"https://api-dev.lightly.ai\"\n",
        "api_workflow_client = ApiWorkflowClient(token=\"d306112ae626405ea7238f22\", dataset_id=\"60f142c8fb1e4200276d6ebf\")\n",
        "api_workflow_client.upload_embeddings(name=\"embedding-1\", path_to_embeddings_csv=path_to_embeddings_csv)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending embeddings from server.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcFmid3Yr7Is"
      },
      "source": [
        "Define the dataset for the classifer, the classifier and the active learning agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# api_workflow_client.upload_embeddings(name=\"embedding-1\", path_to_embeddings_csv=path_to_embeddings_csv)"
      ],
      "metadata": {
        "id": "ykVwib0h3hmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f00Dj1Ger7Is"
      },
      "source": [
        "dataset = CSVEmbeddingDataset(path_to_embeddings_csv=path_to_embeddings_csv)\n",
        "classifier = KNeighborsClassifier(n_neighbors=20, weights='distance')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = ActiveLearningAgent(api_workflow_client=api_workflow_client)"
      ],
      "metadata": {
        "id": "9VaesmFT5dU3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ri434Mr7It"
      },
      "source": [
        "1. Choose an initial subset of your dataset.\n",
        "We want to start with 100 samples and use the CORESET sampler for sampling them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "from lightly.openapi_generated.swagger_client.models.sampling_method import SamplingMethod"
      ],
      "metadata": {
        "id": "JxP4--1ctx_2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelectionConfig:\n",
        "    def __init__(self, method: SamplingMethod = SamplingMethod.CORESET, n_samples: int = 32, min_distance: float = -1,\n",
        "                 name: str = None):\n",
        "\n",
        "        self.method = method\n",
        "        self.n_samples = n_samples\n",
        "        self.min_distance = min_distance\n",
        "        if name is None:\n",
        "            date_time = datetime.now().strftime(\"%m_%d_%Y__%H_%M_%S\")\n",
        "            name = f\"{self.method}_{self.n_samples}_{self.min_distance}_{date_time}\"\n",
        "        self.name = name\n",
        "\n",
        "\n",
        "class SamplingConfig(SelectionConfig):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        warnings.warn(PendingDeprecationWarning(\n",
        "            \"SamplingConfig() is deprecated \"\n",
        "            \"in favour of SelectionConfig() \"\n",
        "            \"and will be removed in the future.\"\n",
        "        ), )\n",
        "        SelectionConfig.__init__(self, *args, **kwargs)\n"
      ],
      "metadata": {
        "id": "wnXFEq6MtuaN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = SelectionConfig(\n",
        "  n_samples=100, \n",
        "  method=SamplingMethod.CORAL, \n",
        "  name='active-learning-loop-12'\n",
        ")\n",
        "agent.query(selection_config=config)"
      ],
      "metadata": {
        "id": "yg3M2Ml8t6UZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN4hRhwjr7It"
      },
      "source": [
        "2. Train a classifier on the labeled set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgStO9wKr7It",
        "outputId": "b8373f1f-ae51-46aa-ff6c-b34c4a93d991"
      },
      "source": [
        "labeled_set_features = dataset.get_features(agent.labeled_set)\n",
        "labeled_set_labels = dataset.get_labels(agent.labeled_set)\n",
        "classifier.fit(X=labeled_set_features, y=labeled_set_labels)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=20, weights='distance')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "622eYDYYr7It"
      },
      "source": [
        "3. Use the classifier to predict on the query set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOw-FUnmr7It"
      },
      "source": [
        "query_set_features = dataset.get_features(agent.query_set)\n",
        "predictions = classifier.predict_proba(X=query_set_features)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2YWXMMrr7Iu"
      },
      "source": [
        "4. Calculate active learning scores from the prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yYOmm17r7Iu"
      },
      "source": [
        "active_learning_scorer = ScorerClassification(model_output=predictions)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CYhCVGar7Iu"
      },
      "source": [
        "5. Use an active learning agent to choose the next samples to be labeled based on the active learning scores.\n",
        "We want to sample another 100 samples to have 200 samples in total and use the active learning sampler CORAL for it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DzlZZtFr7Iu",
        "outputId": "bbdd10a0-e200-4569-dbff-f3d8717eb816"
      },
      "source": [
        "selection_config = SelectionConfig(n_samples=200, method=SamplingMethod.CORAL, name='al-iteration-23')\n",
        "agent.query(selection_config=selection_config, al_scorer=active_learning_scorer)\n",
        "print(f\"There are {len(agent.labeled_set)} samples in the labeled set.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 200 samples in the labeled set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7MWqDjmr7Iu"
      },
      "source": [
        "6. Update the labeled set to include the newly chosen samples and remove them from the unlabeled set.\n",
        "This is already done internally inside the ActiveLearningAgent - no work for you :)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YKHmvMcr7Iu"
      },
      "source": [
        "Now you can use the newly chosen labeled set to retrain you classifer on it.\n",
        "You can evaluate it e.g. on the unlabeled set, or on embeddings of a test set you generated before.\n",
        "If you are not satisfied with the performance, you can run steps 2 to 5 again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz7tYkbOr7Iu",
        "outputId": "d5ecfe9e-c8ad-4437-f016-612bf1267ec2"
      },
      "source": [
        "labeled_set_features = dataset.get_features(agent.labeled_set)\n",
        "labeled_set_labels = dataset.get_labels(agent.labeled_set)\n",
        "classifier.fit(X=labeled_set_features, y=labeled_set_labels)\n",
        "\n",
        "# evaluate on unlabeled set\n",
        "unlabeled_set_features = dataset.get_features(agent.unlabeled_set)\n",
        "unlabeled_set_labels = dataset.get_labels(agent.unlabeled_set)\n",
        "accuracy_on_unlabeled_set = classifier.score(X=unlabeled_set_features, y=unlabeled_set_labels)\n",
        "print(f\"accuracy on unlabeled set: {accuracy_on_unlabeled_set}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on unlabeled set: 0.35704323570432356\n"
          ]
        }
      ]
    }
  ]
}